{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3940381-e5e2-40b7-b0b0-72d4bc9f42da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96fb1822-1894-4512-9ae5-33b525b3805d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 18:52:46.796623: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-12 18:52:46.796694: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.models import models\n",
    "from src.tools import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from transformers import AutoTokenizer, AutoConfig, TFAutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be998f43-c7c5-431b-883f-87ef3e681275",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa757b51-c54e-4892-8c61-0a81e5c9372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data storage\n",
    "data_folder = \"/data\"\n",
    "models_folder = os.path.join(data_folder, \"models\")\n",
    "\n",
    "raw_folder = os.path.join(data_folder, \"raw\")\n",
    "\n",
    "patient_notes_file_path = os.path.join(raw_folder, \"patient_notes.csv\")\n",
    "features_file_path = os.path.join(raw_folder, \"features.csv\")\n",
    "train_file_path = os.path.join(raw_folder, \"train.csv\")\n",
    "test_file_path = os.path.join(raw_folder, \"test.csv\")\n",
    "\n",
    "dataset_columns = [\n",
    "    \"pn_history\",\n",
    "    \"feature_text\",\n",
    "    \"annotation_length\",\n",
    "    \"location\",\n",
    "]\n",
    "\n",
    "# Model params\n",
    "model_name = \"microsoft/deberta-base\"\n",
    "tokenizer_path = os.path.join(models_folder, f\"{model_name}_tokenizer\")\n",
    "batch_size = 8\n",
    "autotune = tf.data.AUTOTUNE\n",
    "epochs = 20\n",
    "model_checkpoint = \"model.h5\"\n",
    "sequence_length = 512\n",
    "learning_rate = 2e-5\n",
    "clip_norm = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c71635a-9246-408b-a764-8e7723c6fd1c",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "953a1003-f122-4bd7-871b-4b089c609aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_notes_df = pd.read_csv(patient_notes_file_path)\n",
    "features_df = pd.read_csv(features_file_path)\n",
    "\n",
    "train_df = pd.read_csv(train_file_path)\n",
    "test_df = pd.read_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6991fa-017d-4554-9d95-04aafe2ba634",
   "metadata": {},
   "source": [
    "### Prepare tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb31b6d5-2a81-403d-838d-33cd53c3e5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(tokenizer_path)\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.save_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2a23b28-391e-4415-ac4e-9c2e09b14b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.create_model(model_name, sequence_length)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ae5372-4612-4d51-b546-491c778ac22d",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5798988c-bb8c-4d5a-a572-89f3fca089e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "      <th>annotation_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>['dad with recent heart attcak']</td>\n",
       "      <td>['696 724']</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>['mom with \"thyroid disease']</td>\n",
       "      <td>['668 693']</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>['chest pressure']</td>\n",
       "      <td>['203 217']</td>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>['intermittent episodes', 'episode']</td>\n",
       "      <td>['70 91', '176 183']</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>['felt as if he were going to pass out']</td>\n",
       "      <td>['222 258']</td>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num  \\\n",
       "0  00016_000         0      16            0   \n",
       "1  00016_001         0      16            1   \n",
       "2  00016_002         0      16            2   \n",
       "3  00016_003         0      16            3   \n",
       "4  00016_004         0      16            4   \n",
       "\n",
       "                                 annotation              location  \\\n",
       "0          ['dad with recent heart attcak']           ['696 724']   \n",
       "1             ['mom with \"thyroid disease']           ['668 693']   \n",
       "2                        ['chest pressure']           ['203 217']   \n",
       "3      ['intermittent episodes', 'episode']  ['70 91', '176 183']   \n",
       "4  ['felt as if he were going to pass out']           ['222 258']   \n",
       "\n",
       "                                        feature_text  \\\n",
       "0  Family-history-of-MI-OR-Family-history-of-myoc...   \n",
       "1                 Family-history-of-thyroid-disorder   \n",
       "2                                     Chest-pressure   \n",
       "3                              Intermittent-symptoms   \n",
       "4                                        Lightheaded   \n",
       "\n",
       "                                          pn_history  annotation_length  \n",
       "0  HPI: 17yo M presents with palpitations. Patien...                 32  \n",
       "1  HPI: 17yo M presents with palpitations. Patien...                 29  \n",
       "2  HPI: 17yo M presents with palpitations. Patien...                 18  \n",
       "3  HPI: 17yo M presents with palpitations. Patien...                 36  \n",
       "4  HPI: 17yo M presents with palpitations. Patien...                 40  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.merge(\n",
    "    train_df, features_df, on=[\"feature_num\", \"case_num\"], how=\"left\")\n",
    "train_df = pd.merge(\n",
    "    train_df, patient_notes_df, on=[\"pn_num\", \"case_num\"], how=\"left\")\n",
    "\n",
    "train_df[\"annotation_length\"] = train_df.annotation.str.len()\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef5078-7a25-4882-8233-7285840d37ea",
   "metadata": {},
   "source": [
    "### Dataset tokenitzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e249262-aaca-475b-8eef-4c4cd165327b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  [(696, 724)]\n",
       "1                  [(668, 693)]\n",
       "2                  [(203, 217)]\n",
       "3        [(70, 91), (176, 183)]\n",
       "4                  [(222, 258)]\n",
       "                  ...          \n",
       "14295                        []\n",
       "14296                        []\n",
       "14297              [(274, 282)]\n",
       "14298              [(421, 437)]\n",
       "14299              [(314, 330)]\n",
       "Name: location, Length: 14300, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.decode_location(train_df.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d39efc1-55ad-4ac8-b271-633ea43467e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inputs_OLD(\n",
    "    pn_history: str, feature_text: str, tokenizer, max_length=512, \n",
    "    padding=\"max_length\", add_special_tokens = True):\n",
    "    \n",
    "    tokens = tokenizer(\n",
    "        pn_history, \n",
    "        feature_text, \n",
    "        max_length=max_length, \n",
    "        padding=padding, \n",
    "        add_special_tokens=add_special_tokens\n",
    "    )\n",
    "    \n",
    "    input_ids = np.array(tokens[\"input_ids\"])\n",
    "    attention_mask = np.array(tokens[\"attention_mask\"])\n",
    "    \n",
    "    return input_ids, attention_mask\n",
    "\n",
    "\n",
    "def decode_location_OLD(location: str): # -> List[Tuple[int]]:\n",
    "    \"\"\"\n",
    "    This function decodes ['ab cd ...'] format of location annotations\n",
    "    from dataset and return list of tuples of locations\n",
    "    \"\"\"\n",
    "    location = location.replace(\"[\", '')\n",
    "    location = location.replace(\"]\", '')\n",
    "    location = location.replace(\"'\", '')\n",
    "    location = location.replace(\",\", '')\n",
    "    location = location.replace(\";\", ' ')\n",
    "    location = location.split(\" \")\n",
    "    if list(filter(None, location)) == []:\n",
    "        return []\n",
    "    \n",
    "    location = list(map(int, location))\n",
    "    location_tuple_list = []\n",
    "    \n",
    "    for i in range(0, len(location), 2):\n",
    "        x1 = location[i]\n",
    "        x2 = location[i+1]\n",
    "        location_tuple_list.append((x1, x2))\n",
    "    \n",
    "    return location_tuple_list\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train\n",
    "def create_labels(pn_history, annotation_length, location_list):\n",
    "    \"\"\"\n",
    "    This function creates labels with are vectors of zeros (no entity)\n",
    "    and ones (entity)\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        pn_history,\n",
    "        add_special_tokens=True,\n",
    "        max_length=sequence_length,\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    \n",
    "    offset_mapping = tokenized[\"offset_mapping\"]\n",
    "    label = np.zeros(len(offset_mapping))\n",
    "    if annotation_length != 0:\n",
    "        locations = decode_location(location_list)\n",
    "        for location in locations:\n",
    "            start_idx, end_idx = -1, -1\n",
    "            start, end = location\n",
    "            for idx in range(len(offset_mapping)):\n",
    "                if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                    start_idx = idx - 1\n",
    "                if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                    end_idx = idx + 1\n",
    "            if start_idx == -1:\n",
    "                start_idx = end_idx\n",
    "            if (start_idx != -1) & (end_idx != -1):\n",
    "                label[start_idx:end_idx] = 1\n",
    "            \n",
    "    return np.array(label)\n",
    "\n",
    "\n",
    "def get_dataset_generator(df: pd.DataFrame):\n",
    "    zipped = zip(\n",
    "        df[\"pn_history\"].values,\n",
    "        df[\"feature_text\"].values, \n",
    "        df[\"feature_text\"].values,\n",
    "        df[\"location\"].values,\n",
    "    )\n",
    "    \n",
    "    for pn_history, feature_text, annotation_length, location in zipped:\n",
    "        inputs, masks = create_inputs(pn_history, feature_text, tokenizer)\n",
    "        labels = create_labels(pn_history, annotation_length, location)\n",
    "        \n",
    "        yield (inputs, masks), labels\n",
    "        \n",
    "        \n",
    "def get_dataloader(dataset_generator) -> tf.data.Dataset:\n",
    "    dataloader = tf.data.Dataset.from_generator(\n",
    "        dataset_generator,\n",
    "        output_signature=(\n",
    "            (\n",
    "                tf.TensorSpec(shape=(sequence_length,), dtype=tf.dtypes.int32, name=\"inputs\"),\n",
    "                tf.TensorSpec(shape=(sequence_length,), dtype=tf.dtypes.int32, name=\"attention_masks\"),\n",
    "            ),\n",
    "            tf.TensorSpec(shape=(sequence_length,), dtype=tf.dtypes.int32, name=\"labels\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    dataloader = dataloader.batch(batch_size)\n",
    "    return dataloader.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a857be0-0851-4a37-ac37-6fbcd41bcecc",
   "metadata": {},
   "source": [
    "### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a735c5b5-d610-40c0-a223-60712dfa294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    train_df[dataset_columns], test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c381c0f-8013-47c2-8130-149d89809e28",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef80db1-ae05-4a3c-b46f-61058d8e483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    min_delta=1e-5, \n",
    "    patience=4, \n",
    "    verbose=1,\n",
    "    mode='auto', \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=1e-5, \n",
    "    patience=2, \n",
    "    mode='auto', \n",
    "    min_delta=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1173bb40-521d-45c7-8bd0-8d70d65a9573",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.f1 = tfa.metrics.F1Score(num_classes=2, average='micro', threshold=0.50)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.reshape(y_true, (-1, sequence_length))\n",
    "        y_pred = tf.reshape(y_pred, (-1, sequence_length))\n",
    "        self.f1.update_state(y_true, y_pred)\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.f1.reset_state()\n",
    "    \n",
    "    def result(self):\n",
    "        return self.f1.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9396e030-8a5c-4e3e-a777-aa6f1b65171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    F1Score(), \n",
    "    tf.keras.metrics.Recall(thresholds=[0.5]), \n",
    "    tf.keras.metrics.Precision(thresholds=[0.5])\n",
    "]\n",
    "\n",
    "callbacks = [reduce_lr, early_stopping]\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, clipnorm=clip_norm)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed936e-7987-47a2-929f-608961f53c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    get_dataloader(lambda: get_dataset_generator(train_df)), \n",
    "    epochs=2,\n",
    "    validation_data=get_dataloader(lambda: get_dataset_generator(val_df)),\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce92dd30-162c-404c-af5c-36a63b7e3143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
